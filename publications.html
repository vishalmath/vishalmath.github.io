<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="src/jemdoc.css" type="text/css" />
<title>Publications &#8201;&mdash;&#8201; Vishal Bhardwaj</title>
<style type="text/css">
.abs {
 background-color: #dae2f1;
 max-width: 800px;
 padding: 7px;
 display: none;
}
li{
    margin-top: 10px;
}
li:first-child {
    margin-top:0;
}
</style>
<script type="text/javascript">
function showAbstract(e){
   f = e;
   var div;
   for(div = e.nextSibling; div.className != "abs"; div = div.nextSibling);
   if (div.style.display=="block"){
     div.style.display="";
   } else {
     div.style.display="block";
   }
   return true;
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
                "HTML-CSS": {
                    availableFonts: ["Asana-Math"],
                    preferredFont: "Asana-Math"
                }
});
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Main</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications &#8201;&mdash;&#8201; Vishal Bhardwaj</h1>
</div>
<p>Authors are in alphabetical order as is the tradition in Theory (exceptions marked by †).</p>
<p>Click on any title to see the corresponding abstract!</p>
<h2>Theses</h2>
<ul>
	
 <li> 
 <a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Some Problems in Number Theory</b></a> [<a href="files/phd-thesis.pdf">pdf</a>]<br/>
Vishal Bhardwaj<br/> 
<b>PhD. Thesis (LSU)</b>, 2027<br/> 
<div class="abs">  
<b>Abstract.</b> ...
	  
</div>  
</li> 		

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Prime Divisors in an Arithmetic Progression and Explicit ABC Conjecture</b></a> [<a href="files/ms_thesis.pdf">pdf</a>]<br/>
Vishal Bhardwaj<br/>
<b>MSc Thesis (NIT Manipur)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> Motivated by the quest for a broader understanding of communication complexity of simple functions, we introduce the class of ``permutation-invariant'' functions. A partial function $f:\{0,1\}^n \times \{0,1\}^n\to \{0,1,?\}$ is permutation-invariant if for every bijection $\pi:\{1,\ldots,n\} \to \{1,\ldots,n\}$ and every $\mathbf{x}, \mathbf{y} \in \{0,1\}^n$, it is the case that $f(\mathbf{x}, \mathbf{y}) = f(\mathbf{x}^{\pi}, \mathbf{y}^{\pi})$. Most of the commonly studied functions in communication complexity are permutation-invariant. For such functions, we present a simple complexity measure (computable in time polynomial in $n$ given an implicit description of $f$) that describes their communication complexity up to polynomial factors and up to an additive error that is logarithmic in the input size. This gives a coarse taxonomy of the communication complexity of simple functions.<br/><br/>
Our work highlights the role of the well-known lower bounds of functions such as <font style="font-variant:small-caps;">Set-Disjointness</font> and <font style="font-variant:small-caps;">Indexing</font>, while complementing them with the relatively lesser-known upper bounds for <font style="font-variant:small-caps;">Gap-Inner-Product</font> (from the sketching literature) and <font style="font-variant:small-caps;">Sparse-Gap-Inner-Product</font> (from the recent work of Canonne et al. [ITCS 2015]). We also present consequences to the study of communication complexity with imperfectly shared randomness where we show that for total permutation-invariant functions, imperfectly shared randomness results in only a polynomial blow-up in communication complexity after an additive $O(\log \log n)$ overhead.
<b>We discussed a brief survey on refinements and generalizations of Sylvester's theorem, which states for consecutive integers states that a product of $k$ consecutive integers, each of which exceeds $k$, is divisible by a prime greater than $k$. These include the statements of some results. Also, We see the $ Baker's$ explicit abc-conjecture. The results for the proof of explicit abc - conjecture is followed to find the values of $\epsilon$, $\omega_{\epsilon}$, and $N_{\epsilon}$</b>
</div>
</li>
	
	
</ul>
<h2>Publications and Pre-prints</h2>
<ul>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Separating Computational and Statistical Differential Privacy (Under Plausible Assumptions)</b></a> [<a href="https://arxiv.org/pdf/2301.00104.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://www.rahulilango.com/">Rahul Ilango</a>,
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<br/>
<b>Manuscript</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> ...
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On Differentially Private Counting on Trees</b></a> [<a href="https://arxiv.org/pdf/2211.11896.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://shlw.github.io/">Kewen Wu</a>
<br/>
<b>Manuscript</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the problem of performing counting queries at different levels in hierarchical structures while preserving individuals' privacy. We propose a new error measure for this setting by considering a combination of multiplicative and additive approximation to the query results. We examine known mechanisms in differential privacy (DP) and prove their optimality in the pure-DP setting. In the approximate-DP setting, we design new algorithms achieving significant improvement over known ones.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Regression with Label Differential Privacy</b></a> [<a href="https://arxiv.org/pdf/2211.11896.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
Ethan Leeman,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
Avinash Varadarajan,
Chiyuan Zhang
<br/>
International Conference on Learning Representations <b>(ICLR)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a "randomized response on bins", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Private Ad Modeling with DP-SGD</b></a> [<a href="https://arxiv.org/pdf/2211.11896.pdf">pdf</a>]<br/>
Carson Denison
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
Krishna Giri Narra,
Amer Sinha,
Avinash Varadarajan,
Chiyuan Zhang
<br/>
AAAI Workshop on Privacy-Preserving Artificial Intelligence <b>(PPAI)</b>, 2023<br/>
<i>(Selected for Long Presentation)</i>
<div class="abs">
<b>Abstract.</b> A well-known algorithm in privacy-preserving ML is differentially private stochastic gradient descent (DP-SGD). While this algorithm has been evaluated on text and image data, it has not been previously applied to ads data, which are notorious for their high class imbalance and sparse gradient updates. In this work we apply DP-SGD to several ad modeling tasks including predicting click-through rates, conversion rates, and number of conversion events, and evaluate their privacyutility trade-off on real-world datasets. Our work is the first to empirically demonstrate that DPSGD can provide both privacy and utility for ad modeling tasks.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Anonymized Histograms in Intermediate Privacy Models</b></a> [<a href="https://arxiv.org/pdf/2210.15178.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We study the problem of privately computing the anonymized histogram (a.k.a. unattributed histogram), which is defined as the histogram without item labels. Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of $O_{\varepsilon}(\sqrt{n})$ in the central model of differential privacy (DP). In this work, we provide an algorithm with a nearly matching error guarantee of $\tilde{O}_{\varepsilon}(\sqrt{n})$ in the shuffle DP and pan-private models. Our algorithm is very simple: it just post-processes the discrete Laplace-noised histogram! Using this algorithm as a subroutine, we show applications in privately estimating symmetric properties of distributions such as entropy, support coverage, and support size.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Private Isotonic Regression</b></a> [<a href="https://arxiv.org/pdf/2210.15175.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> In this paper, we consider the problem of differentially private (DP) algorithms for isotonic regression. For the most general problem of isotonic regression over a partially ordered set (poset) $\mathcal{X}$ and for any Lipschitz loss function, we obtain a pure-DP algorithm that, given $n$ input points, has an expected excess empirical risk of roughly $\mathrm{width}(\mathcal{X}) \cdot \log |\mathcal{X}|/n$, where $\mathrm{width}(X)$ is the width of the poset. In contrast, we also obtain a near-matching lower bound of roughly $(\mathrm{width}(X) + \log |\mathcal{X}|)/n$, that holds even for approximate-DP algorithms. Moreover, we show that the above bounds are essentially the best that can be obtained without utilizing any further structure of the poset. In the special case of a totally ordered set and for $\ell_1$ and $\ell_2^2$ losses, our algorithm can be implemented in near-linear running time; we also provide extensions of this algorithm to the problem of private isotonic regression with additional structural constraints on the output function.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Understanding the Eluder Dimension</b></a> [<a href="https://arxiv.org/pdf/2104.06970.pdf">pdf</a>]<br/>
† <a href="https://gxli97.github.io/">Gene Li</a>,
Pritish Kamath,
<a href="https://dylanfoster.net/">Dylan J. Foster</a>,
<a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We provide new insights on eluder dimension, a complexity measure that has been extensively used to bound the regret of algorithms for online bandits and reinforcement learning with function approximation. First, we study the relationship between the eluder dimension for a function class and a generalized notion of rank, defined for any monotone "activation" $\sigma: \mathbb{R} \to \mathbb{R}$, which corresponds to the minimal dimension required to represent the class as a generalized linear model. It is known that when $\sigma$ has derivatives bounded away from $0$, $\sigma$-rank gives rise to an upper bound on eluder dimension for any function class; we show however that eluder dimension can be exponentially smaller than $\sigma$-rank. We also show that the condition on the derivative is necessary; namely, when $\sigma$ is the relu activation, the eluder dimension can be exponentially larger than $\sigma$-rank. For binary-valued function classes, we obtain a characterization of the eluder dimension in terms of star number and threshold dimension, quantities which are relevant in active learning and online learning respectively.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Do More Negative Samples Necessarily Hurt In Contrastive Learning?</b></a> [<a href="https://arxiv.org/pdf/2205.01789.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=MDfW21AAAAAJ&hl=en">Pranjal Awasthi</a>, 
<a href="https://people.csail.mit.edu/nishanthd/">Nishanth Dikkala</a>, 
Pritish Kamath
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2022<br/>
<i>(Selected for Long Presentation)</i>
<div class="abs">
<b>Abstract.</b> Recent investigations in noise contrastive estimation suggest, both empirically as well as theoretically, that while having more "negative samples" in the contrastive loss improves downstream classification performance initially, beyond a threshold, it hurts downstream performance due to a "collision-coverage" trade-off. But is such a phenomenon inherent in contrastive learning?
We show in a simple theoretical setting, where positive pairs are generated by sampling from the underlying latent class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance of the representation optimizing the (population) contrastive loss in fact does not degrade with the number of negative samples. Along the way, we give a structural characterization of the optimal representation in our framework, for noise contrastive estimation. We also provide empirical support for our theoretical results on CIFAR-10 and CIFAR-100 datasets.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Faster Privacy Accounting via Evolving Discretization</b></a> [<a href="https://arxiv.org/pdf/2207.04381.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a><br/>
International Conference on Machine Learning <b>(ICML)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We introduce a new algorithm for numerical composition of privacy random variables, useful for computing the accurate differential privacy parameters for composition of mechanisms. Our algorithm achieves a running time and memory usage of $\mathrm{polylog}(k)$ for the task of self-composing a mechanism, from a broad class of mechanisms, $k$ times; this class, e.g., includes the sub-sampled Gaussian mechanism, that appears in the analysis of differentially private stochastic gradient descent.
By comparison, recent work by \citet{gopi2021numerical} has obtained a running time of $\widetilde{O}(\sqrt{k})$ for the same task. Our approach extends to the case of composing $k$ different mechanisms in the same class, improving upon their running time and memory usage from $\widetilde{O}(k^{1.5})$ to $\widetilde{O}(k)$.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Connect the Dots: Tighter Discrete Approximations of Privacy Loss Distributions</b></a> [<a href="https://arxiv.org/pdf/2207.04380.pdf">pdf</a>]<br/>
<a href="">Vadym Doroshenko</a>, 
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a><br/>
Privacy Enhancing Technologies Symposium <b>(PETS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> The privacy loss distribution (PLD) provides a tight characterization of the privacy loss of a mechanism in the context of differential privacy (DP). Recent work [Meiser et al. 2018, Koskela et al. 2020, 2021a, 2021b] has shown that PLD-based accounting allows for tighter $(\varepsilon, \delta)$-DP guarantees for many popular mechanisms compared to other known methods. A key question in PLD-based accounting is how to approximate any (potentially continuous) PLD with a PLD over any specified discrete support.<br/><br/>

We present a novel approach to this problem. Our approach supports both <i>pessimistic</i> estimation, which overestimates the hockey-stick divergence (i.e., $\delta$) for any value of $\varepsilon$, and <i>optimistic</i> estimation, which underestimates the hockey-stick divergence. Moreover, we show that our pessimistic estimate is the <i>best</i> possible among all pessimistic estimates. Moreover, this discrete PLD, when used in compositions, would also yield pessimistic/optimistic estimates respectively of the hockey-stick divergence. Experimental evaluation shows that our approach can work with much larger discretization intervals while keeping a similar error bound compared to previous approaches and yet give a better approximation than an existing method [Meiser et al. 2018].
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Circuits Resilient to Short-Circuit Errors</b></a> [<a href="https://eccc.weizmann.ac.il/report/2022/050/download">pdf</a>]<br/>
<a href="https://www.cs.bgu.ac.il/~klim/">Klim Efremenko</a>,
<a href="https://www.cs.cmu.edu/~haeupler/">Bernhard Haeupler</a>,
<a href="https://www.microsoft.com/en-us/research/people/yael/">Yael Tauman Kalai</a>,
Pritish Kamath,
<a href="https://engineering.princeton.edu/faculty/gillat-kol">Gillat Kol</a>,
<a href="https://homepages.cwi.nl/~nar/">Nicolas Resch</a>,
<a href="https://www.microsoft.com/en-us/research/people/rasaxena/">Raghuvansh Saxena</a><br/>
Symposium on Theory of Computing <b>(STOC)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b>  Given a Boolean circuit $C$, we wish to convert it to a circuit $C$ that computes the same function as $C$ even if some of its gates suffer from adversarial short circuit errors, i.e., their output is replaced by the value of one of their inputs [KLM97]. Can we design such a resilient circuit $C$ whose size is roughly comparable to that of $C$? Prior work [KLR12, BEGY19] gave a positive answer for the special case where $C$ is a formula.<br/><br/>

We study the general case and show that any Boolean circuit $C$ of size $s$ can be converted to a new circuit $C'$ of quasi-polynomial size $s^{O(\log s)}$ that computes the same function as $C$ even if a $1/51$ fraction of the gates on any root-to-leaf path in $C'$ are short circuited. Moreover, if the original circuit $C$ is a formula, the resilient circuit $C'$ is of near-linear size $s^{1+\varepsilon}$. The construction of our resilient circuits utilizes the connection between circuits and DAG-like communication protocols [Raz95, Pud10, Sok17], originally introduced in the context of proof complexity.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On the Power of Differentiable Learning versus PAC and SQ Learning</b></a> [<a href="https://arxiv.org/pdf/2108.04190.pdf">pdf</a>]<br/>
<a href="https://mds.epfl.ch/">Emmanuel Abbe</a>, <a href="https://www.eranmalach.com/">Eran Malach</a>, Pritish Kamath, <a href="">Colin Sandon</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2021<br/>
<i>(Selected for Spotlight presentation)</i>
<div class="abs">
<b>Abstract.</b> We study the power of learning via mini-batch stochastic gradient descent (SGD) on the population loss, and batch Gradient Descent (GD) on the empirical loss, of a differentiable model or neural network, and ask what learning problems can be learnt using these paradigms. We show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision $\rho$ of the gradient calculations relative to the minibatch size $b$ (for SGD) and sample size $m$ (for GD). With fine enough precision relative to minibatch size, namely when $b\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for $b=1$. Similarly, with fine enough precision relative to the sample size $m$, GD can also simulate any sample-based learning algorithm based on $m$ samples. In particular, with polynomially many bits of precision (i.e. when $\rho$ is exponentially small), SGD and GD can both simulate PAC learning regardless of the mini-batch size. On the other hand, when $b\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels</b></a> [<a href="https://arxiv.org/pdf/2103.01210.pdf">pdf</a>]<br/>
† <a href="https://www.eranmalach.com/">Eran Malach</a>, Pritish Kamath, <a href="https://mds.epfl.ch/">Emmanuel Abbe</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
International Conference on Machine Learning <b>(ICML)</b>, 2021<br/>
<div class="abs">
<b>Abstract.</b> We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Does Invariant Risk Minimization Capture Invariance?</b></a> [<a href="https://arxiv.org/pdf/2101.01134.pdf">pdf</a>]<br/>
† Pritish Kamath, <a>Akilesh Tangella</a>, <a href="https://djsutherland.ml/">Danica J. Sutherland</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Artificial Intelligence and Statistics Conference <b>(AISTATS)</b>, 2021<br/>
<i>(Selected for Oral presentation)</i>
<div class="abs">
<b>Abstract.</b> We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Approximate is Good Enough: Probabilistic Variants of Dimensional and Margin Complexity</b></a> [<a href="https://arxiv.org/pdf/2003.04180.pdf">pdf</a>]<br/>
Pritish Kamath, <a href="https://ttic.uchicago.edu/~omar/">Omar Montasser</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Conference on Learning Theory <b>(COLT)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> We present and study approximate notions of dimensional and margin complexity, which correspond to the minimal dimension or norm of an embedding required to approximate, rather then exactly represent, a given hypothesis class. We show that such notions are not only sufficient for learning using linear predictors or a kernel, but unlike the exact variants, are also necessary. Thus they are better suited for discussing limitations of linear or kernel methods.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Optimality of Correlated Sampling Strategies</b></a> [<a href="https://arxiv.org/pdf/1612.01041.pdf">pdf</a>] [<a href="https://theoryofcomputing.org/articles/v016a012/">ToC version</a>]<br/>
<a href="http://bavarian.mit.edu/">Mohammad Bavarian</a>, <a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Elad Haramaty, Pritish Kamath, <a href="https://people.csail.mit.edu/rivest/">Ronald L. Rivest</a>, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Theory of Computing <b>(TOC)</b>, 2020
<div class="abs">
<b>Abstract.</b>
In the <i>correlated sampling</i> problem, two players are given probability distributions $P$ and $Q$, respectively, over the same finite set, with access to shared randomness. Without any communication, the two players are each required to output an element sampled according to their respective distributions, while trying to minimize the probability that their outputs disagree. A well known strategy due to Kleinberg-Tardos and Holenstein, with a close variant (for a similar problem) due to Broder, solves this task with disagreement probability at most $2\delta/(1+\delta)$, where $\delta$ is the total variation distance between $P$ and $Q$. This strategy has been used in several different contexts, including sketching algorithms, approximation algorithms based on rounding linear programming relaxations, the study of parallel repetition and cryptography.<br/><br/>

In this paper, we give a surprisingly simple proof that this strategy is essentially optimal. Specifically, for every $\delta \in (0,1)$, we show that any correlated sampling strategy incurs a disagreement probability of essentially $2\delta/(1+\delta)$ on some inputs $P$ and $Q$ with total variation distance at most $\delta$. This partially answers a recent question of Rivest.<br/><br/>

Our proof is based on studying a new problem that we call <i>constrained agreement</i>. Here, the two players are given subsets $A \subseteq [n]$ and $B \subseteq [n]$, respectively, and their goal is to output an element $i \in A$ and $j \in B$, respectively, while minimizing the probability that $i \ne j$. We prove tight bounds for this question, which in turn imply tight bounds for correlated sampling. Though we settle basic questions about the two problems, our formulation leads to more fine-grained questions that remain open.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On the Complexity of Modulo-$q$ Arguments and the Chevalley-Warning Theorem</b></a> [<a href="https://arxiv.org/abs/1912.04467">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="http://www.mit.edu/~katesot/">Katerina Sotiraki</a>, <a href="http://www.mit.edu/~mzampet/">Manolis Zampetakis</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> We study the search problem class $\mathsf{PPA}_q$ defined as a modulo-$q$ analog of the well-known <i>polynomial parity argument</i> class $\mathsf{PPA}$ introduced by Papadimitriou '94. Our first result shows that this class can be characterized in terms of $\mathsf{PPA}_p$ for prime $p$.<br/><br/>
Our main result is to establish that an <i>explicit</i> version of a search problem associated to the Chevalley--Warning theorem is complete for $\mathsf{PPA}_p$ for prime $p$. This problem is <i>natural</i> in that it does not explicitly involve circuits as part of the input. It is the first such complete problem for $\mathsf{PPA}_p$ when $p \ge 3$.<br/><br/>
Finally we discuss connections between Chevalley-Warning theorem and the well-studied <i>short integer solution</i> problem and survey the structural properties of $\mathsf{PPA}_q$. 
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Limits on the Efficiency of (Ring) LWE based Non-Interactive Key Exchange</b></a> [<a href="files/GKRS_Key-Exchange-LWE.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/site/siyaoguo/">Siyao Guo</a>, Pritish Kamath, <a href="https://www.alonrosen.net/">Alon Rosen</a>, <a href="http://www.mit.edu/~katesot/">Katerina Sotiraki</a><br/>
Public Key Cryptography <b>(PKC)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> <br/>
$\mathsf{LWE}$ based key-exchange protocols lie at the heart of post-quantum public-key cryptography. However, all existing protocols either lack the <i>non-interactive</i> nature of Diffie-Hellman key-exchange or <i>polynomial</i> $\mathsf{LWE}$-modulus, resulting in unwanted efficiency overhead.

We study the possibility of designing non-interactive $\mathsf{LWE}$-based protocols with <i>polynomial</i> $\mathsf{LWE}$-modulus. To this end,<br/><br/>
<ul>
<li> We identify and formalize simple non-interactive and polynomial $\mathsf{LWE}$-modulus variants of existing protocols, where Alice and Bob <i>simultaneously</i> exchange one or more (ring) $\mathsf{LWE}$ samples with polynomial $\mathsf{LWE}$-modulus and then run individual key reconciliation functions to obtain the shared key.
<li>  We point out central barriers and show that such non-interactive key-exchange protocols are impossible if:<br/><br/>
	<ol>
	<li> the reconciliation functions first compute the inner product of the received $\mathsf{LWE}$ sample with their private $\mathsf{LWE}$ secret. This impossibility is information theoretic.</li>
	<li> One of the reconciliation functions does not depend on the error of the transmitted $\mathsf{LWE}$ sample. This impossibility assumes hardness of $\mathsf{LWE}$.</li>
	</ol>
<li>We give further evidence that progress in either direction, of giving an $\mathsf{LWE}$-based non-interactive key exchange protocol or proving impossibility of one will lead to progress on some other well-studied questions in cryptography.</li>
</ul>
 Overall, our results show possibilities and challenges in designing simple (ring) $\mathsf{LWE}$-based non-interactive key exchange protocols.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Adventures in Monotone Complexity and $\mathsf{TFNP}$</b></a> [<a href="https://eccc.weizmann.ac.il/report/2018/163/download">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://www.cs.toronto.edu/~robere/">Robert Robere</a>, <a href="https://logic.pdmi.ras.ru/~sokolov/">Dmitry Sokolov</a><br/>
Innovations in Theoretical Computer Science <b>(ITCS)</b>, 2019<br/>
<i>(invited talk at <a href="http://www.mit.edu/~mzampet/workshop/tfnp.html">FOCS 2018 workshop on TFNP</a>)</i>
<div class="abs">
<b>Abstract.</b> <br/>
<i>Separations:</i> We introduce a monotone variant of $\mathrm{Xor}\text{-}\mathrm{SAT}$ and show it has exponential monotone circuit complexity. Since $\mathrm{Xor}\text{-}\mathrm{SAT}$ is in $\mathsf{NC}^2$, this improves qualitatively on the monotone vs. non-monotone separation of Tardos (1988). We also show that monotone span programs over $\mathbb{R}$ can be exponentially more powerful than over finite fields. These results can be interpreted as separating subclasses of $\mathsf{TFNP}$ in communication complexity.<br/><br/>

<i>Characterizations:</i> We show that the communication (resp. query) analogue of $\mathsf{PPA}$ (subclass of $\mathsf{TFNP}$) captures span programs over $\mathbb{F}_2$ (resp. Nullstellensatz degree over $\mathbb{F}_2$). Previously, it was known that communication $\mathsf{FP}$ captures formulas (Karchmer-Wigderson, 1988) and that communication $\mathsf{PLS}$ captures circuits (Razborov, 1995).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Bayesian Inference of Temporal Task Specifications from Demonstrations</b></a> [<a href="https://interactive.mit.edu/sites/default/files/documents/nips_2018_preprint.pdf">pdf</a>]<br/>
† <a href="http://interactive.mit.edu/about/people/ankit">Ankit Shah</a>, Pritish Kamath, <a href="http://interactive.mit.edu/about/people/shen">Shen Li</a>, <a href="http://interactive.mit.edu/about/people/julie">Julie Shah</a><br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2018
<div class="abs">
<b>Abstract.</b> When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Monotone Circuit Lower Bounds from Resolution</b></a> [<a href="https://eccc.weizmann.ac.il/report/2017/175/download">pdf</a>]<br/>
<a href="https://www.microsoft.com/en-us/research/people/garga/">Ankit Garg</a>, <a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://logic.pdmi.ras.ru/~sokolov/">Dmitry Sokolov</a><br/>
Symposium on Theory Of Computing <b>(STOC)</b>, 2018
<div class="abs">
<b>Abstract.</b> For any unsatisfiable CNF formula $F$ that is hard to refute in the Resolution proof system, we show that a gadget-composed version of $F$ is hard to refute in any proof system whose lines are computed by efficient communication protocols---or, equivalently, that a monotone function associated with $F$ has large monotone circuit complexity. Our result extends to monotone real circuits, which yields new lower bounds for the Cutting Planes proof system.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Dimension Reduction for Polynomials over Gaussian Space and Applications</b></a> [<a href="files/GKR_DimRedPolys.pdf">shorter</a>][<a href="https://eccc.weizmann.ac.il/report/2017/125/download">longer</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://www.eecs.berkeley.edu/~prasad/">Prasad Raghavendra</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2018
<div class="abs">
<b>Abstract.</b> We introduce a new technique for reducing the dimension of the ambient space of low-degree polynomials in the Gaussian space while preserving their relative correlation structure. As an application, we obtain an explicit upper bound on the dimension of an $\varepsilon$-optimal noise-stable Gaussian partition. In fact, we address the more general problem of upper bounding the number of samples needed to $\varepsilon$-approximate any joint distribution that can be <i>non-interactively simulated</i> from a correlated Gaussian source.  Our results significantly improve (from Ackermann-like to "merely" exponential) the upper bounds recently proved on the above problems by De, Mossel & Neeman (CCC 2017, SODA 2018 resp.), and imply decidability of the larger alphabet case of the <i>gap non-interactive simulation problem</i> posed by Ghazi, Kamath & Sudan (FOCS 2016).<br/><br/>

Our technique of dimension reduction for low-degree polynomials is simple and can be seen as a generalization of the Johnson-Lindenstrauss lemma and could be of independent interest.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Query-to-Communication Lifting for $\mathrm{P}^{\mathrm{NP}}$</b></a> [<a href="https://eccc.weizmann.ac.il/report/2017/024/download/">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://www.cs.toronto.edu/~toni/">Toniann Pitassi</a>, <a href="http://umdrive.memphis.edu/twwtson1/public/">Thomas Watson</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2017
<div class="abs">
<b>Abstract.</b> We prove that the $\mathrm{P}^{\mathrm{NP}}$-type query complexity (alternatively, decision list width) of any boolean function $f$ is quadratically related to the $\mathrm{P}^{\mathrm{NP}}$-type communication complexity of a lifted version of $f$. As an application, we show that a certain "product" lower bound method of Impagliazzo and Williams (CCC 2010) fails to capture $\mathrm{P}^{\mathrm{NP}}$ communication complexity up to polynomial factors, which answers a question of Papakonstantinou, Scheder, and Song (CCC 2014). 
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Improved Bounds for Universal One-Bit Compressive Sensing</b></a> [<a href="https://arxiv.org/abs/1705.00763">pdf</a>]<br/>
<a href="http://people.ece.cornell.edu/acharya/">Jayadev Acharya</a>, <a href="http://drona.csa.iisc.ernet.in/~arnabb/">Arnab Bhattacharyya</a>, Pritish Kamath<br/>
International Symposium on Information Theory <b>(ISIT)</b>, 2017
<div class="abs">
<b>Abstract.</b> Unlike compressive sensing where the measurement outputs are assumed to be real-valued and have infinite precision, in <i>one-bit compressive sensing</i>, measurements are quantized to one bit, their signs. In this work, we show how to recover the support of sparse high-dimensional vectors in the one-bit compressive sensing framework with an asymptotically near-optimal number of measurements. We also improve the bounds on the number of measurements for approximately recovering vectors from one-bit compressive sensing measurements. Our results are universal, namely the same measurement scheme works simultaneously for all sparse vectors.<br/><br/>

Our proof of optimality for support recovery is obtained by showing an equivalence between the task of support recovery using 1-bit compressive sensing and a well-studied combinatorial object known as Union Free Families.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Compression in a Distributed Setting</b></a> [<a href="files/GHKS_distributed_compression.pdf">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Elad Haramaty, Pritish Kamath, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Innovations in Theoretical Computer Science <b>(ITCS)</b>, 2017
<div class="abs">
<b>Abstract.</b> Motivated by an attempt to understand the formation and development of (human) language, we introduce a "distributed compression" problem. In our problem a sequence of pairs of players from a set of $K$ players are chosen and tasked to communicate messages drawn from an unknown distribution $Q$. Arguably languages are created and evolve to compress frequently occurring messages, and we focus on this aspect. The only knowledge that players have about the distribution $Q$ is from previously drawn samples, but these samples differ from player to player. The only <i>common</i> knowledge between the players is restricted to a common prior distribution $P$ and some constant number of bits of information (such as a learning algorithm). Letting $T_\varepsilon$ denote the number of iterations it would take for a typical player to obtain an $\epsilon$-approximation to $Q$ in total variation distance, we ask whether $T_\varepsilon$ iterations suffice to compress the messages down roughly to their entropy and give a partial positive answer.<br/><br/>

We show that a natural uniform algorithm can compress the communication down to an average cost per message of $O(H(Q) + \log (D(P || Q) + O(1))$ in $\widetilde{O}(T_\varepsilon)$ iterations while allowing for $O(\varepsilon)$-error, where $D(\cdot || \cdot)$ denotes the KL-divergence between distributions. For large divergences this compares favorably with the static algorithm that ignores all samples and compresses down to $H(Q) + D(P || Q)$ bits, while not requiring $T_\varepsilon\cdot K$ iterations that it would take players to develop optimal but separate compressions for each pair of players. Along the way we introduce a "data-structural" view of the task of communicating with a natural language and show that our natural algorithm can also be implemented by an efficient data structure, whose storage is comparable to the storage requirements of $Q$ and whose query complexity is comparable to the lengths of the message to be compressed. Our results give a plausible mathematical analogy to the mechanisms by which human languages get created and evolve, and in particular highlights the possibility of coordination towards a joint task (agreeing on a language) while engaging in distributed learning.
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Decidability of Non-Interactive Simulation of Joint Distributions</b></a> [<a href="http://eccc.weizmann.ac.il/report/2016/104/download">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Foundations of Computer Science <b>(FOCS)</b>, 2016
<div class="abs">
<b>Abstract.</b> We present decidability results for a sub-class of "non-interactive" simulation problems, a well-studied class of problems in information theory. A <i>non-interactive simulation</i> problem is specified by two distributions $P(x,y)$ and $Q(u,v)$: The goal is to determine if two players, Alice and Bob, that observe sequences $X^n$ and $Y^n$ respectively, where $\{(X_i, Y_i)\}_{i=1}^n$ are drawn i.i.d. from $P(x,y)$, can generate pairs $U$ and $V$ respectively (without communicating with each other) with a joint distribution that is arbitrarily close in total variation to $Q(u,v)$. Even when $P$ and $Q$ are extremely simple: e.g., $P$ is uniform on the triples $\{(0,0), (0,1), (1,0)\}$ and $Q$ is a "doubly symmetric binary source", i.e., $U$ and $V$ are uniform $\pm 1$ variables with correlation say $0.49$, it is open if $P$ can simulate $Q$.<br/><br/>

In this work, we show that whenever $P$ is a distribution on a finite domain and $Q$ is a $2 \times 2$ distribution, then the non-interactive simulation problem is <i>decidable</i>: specifically, given $\delta > 0$ the algorithm runs in time bounded by some function of $P$ and $\delta$ and either gives a non-interactive simulation protocol that is $\delta$-close to $Q$ or asserts that no protocol gets $O(\delta)$-close to $Q$. The main challenge to such a result is determining explicit (computable) convergence bounds on the number $n$ of samples that need to be drawn from $P(x,y)$ to get $\delta$-close to $Q$. We invoke contemporary results from the analysis of Boolean functions such as the invariance principle and a regularity lemma to obtain such explicit bounds.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Communication complexity of permutation-invariant functions</b></a> [<a href="http://eccc.weizmann.ac.il/report/2015/087/download">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://madhu.seas.harvard.edu">Madhu Sudan</a><br/>
Symposium On Discete Algorithms <b>(SODA)</b>, 2016
<div class="abs">
<b>Abstract.</b> Motivated by the quest for a broader understanding of upper bounds in communication complexity, at least for simple functions, we introduce the class of ''permutation-invariant'' functions. A partial function $f:\{0,1\}^n \times \{0,1\}^n\to \{0,1,?\}$ is permutation-invariant if for every bijection $\pi:\{1,\ldots,n\} \to \{1,\ldots,n\}$ and every $\mathbf{x}, \mathbf{y} \in \{0,1\}^n$, it is the case that $f(\mathbf{x}, \mathbf{y}) = f(\mathbf{x}^{\pi}, \mathbf{y}^{\pi})$. Most of the commonly studied functions in communication complexity are permutation-invariant. For such functions, we present a simple complexity measure (computable in time polynomial in $n$ given an implicit description of $f$) that describes their communication complexity up to polynomial factors and up to an additive error that is logarithmic in the input size. This gives a coarse taxonomy of the communication complexity of simple functions.<br/><br/>

Our work highlights the role of the well-known lower bounds of functions such as 'Set-Disjointness' and 'Indexing', while complementing them with the relatively lesser-known upper bounds for 'Gap-Inner-Product' (from the sketching literature) and 'Sparse-Gap-Inner-Product' (from the recent work of Canonne et al. [ITCS 2015]). We also present consequences to the study of communication complexity with imperfectly shared randomness where we show that for total permutation-invariant functions, imperfectly shared randomness results in only a polynomial blow-up in communication complexity after an additive $O(\log \log n)$ overhead.
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Communication with partial noiseless feedback</b></a> [<a href="http://drops.dagstuhl.de/opus/volltexte/2015/5342/pdf/52.pdf">pdf</a>]<br/>
<a href="http://www.cs.cmu.edu/~haeupler">Bernhard Haeupler</a>, Pritish Kamath, <a href="http://www.cs.cmu.edu/~avelingk">Ameya Velingker</a><br/>
International Workshop on Randomization and Computation <b>(RANDOM)</b>, 2015
<div class="abs">
<b>Abstract.</b> We introduce the notion of <i>one-way communication schemes with partial noiseless feedback</i>. In this setting, Alice wishes to communicate a message to Bob by using a communication scheme that involves sending a sequence of bits over a channel while receiving feedback bits from Bob for $\delta$ fraction of the transmissions. An adversary is allowed to corrupt up to a constant fraction of Alice's transmissions, while the feedback is always uncorrupted. Motivated by questions related to coding for interactive communication, we seek to determine the maximum error rate, as a function of $0 \le \delta \le 1$, such that Alice can send a message to Bob via some protocol with $\delta$ fraction of noiseless feedback. The case $\delta = 1$ corresponds to <i>full feedback</i>, in which the result of Berlekamp ['64] implies that the maximum tolerable error rate is $1/3$, while the case $\delta = 0$ corresponds to <i>no feedback</i>, in which the maximum tolerable error rate is $1/4$, achievable by use of a binary error-correcting code.<br/><br/>

In this work, we show that for any $\delta \in (0,1]$ and $\gamma \in [0, 1/3)$, there exists a <i>randomized</i> communication scheme with noiseless $\delta$-feedback, such that the probability of miscommunication is low, as long as no more than a $\gamma$ fraction of the rounds are corrupted. Moreover, we show that for any $\delta \in (0, 1]$ and $\gamma < f(\delta)$, there exists a <i>deterministic</i> communication scheme with noiseless $\delta$-feedback that always decodes correctly as long as no more than a $\gamma$ fraction of rounds are corrupted. Here $f$ is a monotonically increasing, piecewise linear, continuous function with $f(0) = 1/4$ and $f(1) = 1/3$. Also, the rate of communication in both cases is constant (dependent on $\delta$ and $\gamma$ but independent of the input length).
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Arithmetic Circuits : A chasm at depth three</b></a> [<a href="http://eccc.weizmann.ac.il/report/2013/026/revision/1/download">pdf</a>] [<a href="files/kamath_chasm_at_3.pdf">FOCS slides</a>] [<a href="http://techtalks.tv/talks/arithmetic-circuits-a-chasm-at-depth-three/59344/">FOCS video</a>] [<a href="http://www.youtube.com/watch?v=WsO2V4t0HtY">TCS+ video</a>]<br/>
<a href="https://sites.google.com/site/ankitgupta1988site/home/">Ankit Gupta</a>, Pritish Kamath, <a href="http://research.microsoft.com/en-us/people/neeraka/">Neeraj Kayal</a>, <a href="http://www.cmi.ac.in/~ramprasad/">Ramprasad Saptharishi</a><br/>
Foundations of Computer Science <b>(FOCS)</b>, 2013 (also in <b>SICOMP</b> and <b>CACM</b>)
<div class="abs">
<b>Abstract.</b> We show that, over $\mathbb{Q}$, if an $n$-variate polynomial of degree $d = n^{O(1)}$ is computable by an arithmetic circuit of size $s$ (respectively by an algebraic branching program of size $s$) then it can also be computed by a depth three circuit (i.e. a $\Sigma \Pi \Sigma$-circuit) of size $\exp(O(\sqrt{d \log d \log n \log s}))$ (respectively of size $\exp(O(\sqrt{d \log n \log s}))$). In particular this yields a $\Sigma \Pi \Sigma$ circuit of size $\exp(O(\sqrt{d} \cdot \log d))$ computing the $d \times d$ determinant $\mathrm{Det}_d$. It also means that if we can prove a lower bound of $\exp(\omega(\sqrt{d} \cdot \log^{3/2} d))$ on the size of any $\Sigma \Pi \Sigma$-circuit computing the $d \times d$ permanent ${\rm Perm}_d$ then we get superpolynomial lower bounds for the size of any arithmetic circuit computing ${\rm Perm}_d$.  We then give some further results pertaining to derandomizing polynomial identity testing and circuit lower bounds.<br/><br/>
  
The $\Sigma \Pi \Sigma $ circuits that we construct have the property that (some of) the intermediate polynomials have degree much higher than $d$. Indeed such a counterintuitive construction is unavoidable - it is known that in any $\Sigma \Pi \Sigma$ circuit $C$ computing either ${\rm Det}_d$ or ${\rm Perm}_d$, if every multiplication gate has fanin at most $d$ (or any constant multiple thereof) then $C$ must have size at least $\exp(\Omega(d))$.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Approaching the chasm at depth four</b></a> [<a href="http://eccc.weizmann.ac.il/report/2012/098/revision/3/download">pdf</a>]<br/>
<a href="https://sites.google.com/site/ankitgupta1988site/home/">Ankit Gupta</a>, Pritish Kamath, <a href="http://research.microsoft.com/en-us/people/neeraka/">Neeraj Kayal</a>, <a href="http://www.cmi.ac.in/~ramprasad/">Ramprasad Saptharishi</a><br/>
Conference on Computational Complexity <b>(CCC)</b>, 2013 (also in <b>J.ACM</b>)<br/>
<i>(co-winner of the Best Paper Award)</i><br/>
<div class="abs">
<b>Abstract.</b> Agrawal-Vinay (FOCS 2008), Koiran (TCS 2012) and Tavenas (MFCS 2012) have recently shown that an $\exp(\omega(\sqrt{n}\log n))$ lower bound for depth four homogeneous circuits computing the permanent with bottom layer of $\times$ gates having fanin bounded by $\sqrt{n}$ translates to super-polynomial lower bound for general arithmetic circuits computing the permanent. Motivated by this, we examine the complexity of computing the permanent and determinant via such homogeneous depth four circuits with bounded bottom fanin.<br/><br/>
  
We show here that any homogeneous depth four arithmetic circuit with bottom fanin bounded by $\sqrt{n}$ computing the permanent (or the determinant) must be of size $\exp(\Omega(\sqrt{n}))$.
</div>
</li>
</ul>
<h2>Before 2013 (Undergraduate Research)</h2>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Preservation under substructures modulo bounded cores</b></a> [<a href="http://rd.springer.com/chapter/10.1007%2F978-3-642-32621-9_22">pdf</a>]<br/>
† Abhisekh Sankaran, <a href="http://www.cse.iitb.ac.in/~adsul/">Bharat Adsul</a>, Vivek Madan, Pritish Kamath, <a href="http://www.cse.iitb.ac.in/~supratik/">Supratik Chakraborty</a><br/>
International Workshop on Logic, Language, Information and Computation <b>(WoLLIC)</b>, 2012<br/>
<div class="abs">
<b>Abstract.</b> We investigate a model-theoretic property that generalizes the classical notion of preservation under substructures. We call this property preservation under substructures modulo bounded cores, and present a syntactic characterization via $\Sigma^0_2$ sentences for properties of arbitrary structures definable by FO sentences. Towards a sharper characterization, we  conjecture  that the  count of existential quantifiers in the $\Sigma^0_2$ sentence equals the size of the smallest bounded core. We show that this conjecture holds for special fragments of FO and also over special classes of structures. We present a (not FO-definable) class of finite structures for which the conjecture fails, but for which the classical Łoś-Tarski preservation theorem holds. As a fallout of our studies, we obtain combinatorial proofs of the Łoś-Tarski theorem for some of the aforementioned cases.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Faster algorithms for alternating refinement relations</b></a> [<a href="http://drops.dagstuhl.de/opus/volltexte/2012/3671/">pdf</a>]<br/>
<a href="http://pub.ist.ac.at/~kchatterjee/"> Krishnendu Chatterjee</a>, Siddhesh Chaubal, Pritish Kamath<br/>
Computer Science and Logic <b>(CSL)</b>, 2012<br/>
<div class="abs">
<b>Abstract.</b> One central issue in the formal design and analysis of reactive systems is the notion of refinement that asks whether all behaviors of the implementation is allowed by the specification. The local interpretation of behavior leads to the notion of simulation. Alternating transition systems (ATSs) provide a general model for composite reactive systems, and the simulation relation for ATSs is known as alternating simulation. The simulation relation for fair transition systems is called fair simulation. In this work our main contributions are as follows:<br/><br/>
<ol>
<li> We present an improved algorithm for fair simulation with Büchi fairness constraints; our algorithm requires $O(n^3 \cdot m)$ time as compared to the previous known $O(n^6)$-time algorithm, where $n$ is the number of states and $m$ is the number of transitions.</li>

<li>We present a game based algorithm for alternating simulation that requires $O(m^2)$-time as compared to the previous known $O((n \cdot m)^2)$-time algorithm, where $n$ is the number of states and $m$ is the size of transition relation.</li>

<li>We present an iterative algorithm for alternating simulation that matches the time complexity of the game based algorithm, but is more space efficient than the game based algorithm.</li>
</ol>
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Using dominances for solving the protein family identification problem</b></a> [<a href="http://hal.inria.fr/inria-00609432">pdf</a>]<br/>
† <a href="http://team.inria.fr/abs/team-members/homepage-noel-malod-dognin/"> Noël Malod-Dognin</a>, Mathilde Le Boudic-Jamin, Pritish Kamath, <a href="http://www.irisa.fr/symbiose/rumen_andonov">Rumen Andonov</a><br/>
Workshop on Algorithms in Bioinformatics <b>(WABI)</b>, 2011<br/>
<div class="abs">
Identification of protein families is a computational biology challenge that needs efficient and reliable methods. Here we introduce the concept of dominance and propose a novel combined approach based on Distance Alignment Search Tool (DAST), which contains an exact algorithm with bounds. Our experiments show that this method successfully finds the most similar proteins in a set without solving all instances.
</div>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Last updated on 2023-04-02. Originally created by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
